---
title: "Activation Functions and Neural Networks"
description: "Explore the key roles of activation functions in neural networks for computer vision tasks. Learn about Sigmoid, ReLU, and more for better deep learning."
date: "2023-08-31"
author:
  name: "Picsellia Team"
category: "Data Science"
image: "/images/blog/activation-functions-and-neural-networks-hero.png"
published: true
---

## Introduction

Let's say we want to predict the finalists of the Rugby World Cup. Instead of relying solely on human analysis, we can use a Deep Learning model.

The model would be trained on historical data of Rugby World Cup matches, including team performances and outcomes. By applying successive transformations to the input data using activation functions, the model learns complex patterns and relationships.

Once trained, the model takes in the recent performances of participating teams as input. It uses activation functions to process the data and make predictions about the two teams most likely to reach the finals.

This article aims to explore various activation functions commonly employed in computer vision tasks. We will delve into their properties, discuss advantages and limitations, and provide specific examples to illustrate their applications in different scenarios.

## Understanding Neural Networks

Before we dive into the intricacies of activation functions, let's briefly recap the concept of neural networks in computer vision. Neural networks, inspired by the complexity of the human brain, consist of interconnected layers of artificial neurons. There is an input and output layer, linked by hidden layers as shown on the figure.  Each hidden layer is made with neurons connected to one or several neurons from the previous and following layer. Neurons are actually function. 

![](/images/blog/activation-functions-and-neural-networks-img-0.png)
